import pandas as pd
import re
import unidecode
from sqlalchemy import create_engine
from dotenv import load_dotenv
import os

class Data_Etl:
    def __init__(self, path_data, postgres_db):
        self.path_data = path_data
        self.engine = create_engine(postgres_db)

    # Penso que esta função é responsável por demasiada coisa
    # Devias delegar todas as transformações noutros métodos da classe.
    def load_data(self, table_name):
        """
        Loads data from a CSV file and ensures proper data types.
        Parses any column with 'date' in the name as type datetime,
        Cast numeric columns to floats, and all other columns to strings,
        ignoring null values and ensuring non-numeric columns (e.g., names, mixed alphanumerics) stay as strings.
        """
        file_path = f'{self.path_data}/{table_name}'
        df = pd.read_csv(file_path)

        # Identify  columns with 'date' in their name and convert them to datetime
        date_columns = [col for col in df.columns if 'date' in col.lower()]
        df[date_columns] = df[date_columns].apply(pd.to_datetime, errors='coerce')

        # Verify is exist any alpha in  the column, if yes, convert to string
        def contains_alpha(val):
            if isinstance(val, str):
                return bool(re.search(r'[a-zA-Z]', val))  # verify if exist letters
            return False

        for col in df.columns:
            if col in date_columns:
                continue

            # Apply the  function to each value in the column
            if df[col].dropna().apply(contains_alpha).any():
                df[col] = df[col].astype(str)  # if is function return true, convert to string
            else:
                # if function is false, convert into numeric values
                if df[col].notnull().any():
                    try:
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                    except ValueError:
                        pass 

            if df[col].dtype == 'object':
                df[col] = df[col].where(df[col].isnull(), df[col].astype(str))  

        return df
    def replaceNA(self, df):
        """ 
        If the dtype is numeric, fill NaN with 0. 
        If the dtype is string, fill NaN with 'don't exist'. 
        If the dtype is a date (object), fill NaN with '01/01/1990' and convert to datetime.
        """
        for column in df.columns:
            if pd.api.types.is_numeric_dtype(df[column]):
                df[column] = df[column].fillna(0.0)
            elif pd.api.types.is_string_dtype(df[column]):
                df[column] = df[column].fillna("don't exist")
            elif pd.api.types.is_object_dtype(df[column]):  
                try:
                    df[column] = pd.to_datetime(df[column], errors='coerce')  
                    df[column] = df[column].fillna(pd.to_datetime("1990-01-01"))  
                except Exception as e:
                    print(f"Error converting {column}: {e}")
        return df


    def clean_special_characters(self, df):
        """
        Cleans the string columns by removing special characters
        Keeps exceptions for numerical and date types
        """
        def replace_accented_characters(text):
            """
            Replaces accented characters
            Removes special characters and Title case
            """
            if not isinstance(text, str):
                return text
            
            # Remove accented characters 
            text = unidecode.unidecode(text)
            
            # Remove special characters (only stay alpha, numeric an symbols)
            text = re.sub(r'[^a-zA-Z0-9 ]', '', text).title().strip()
            return text

        # call the function replace_accented_characters
        for column in df.select_dtypes(include='object').columns:
            df[column] = df[column].apply(replace_accented_characters)

        return df
    
    def load_to_postgres(self, df, table_name):
        """
        Loads a pandas DataFrame to PostgreSQL.
        """
        df.to_sql(table_name, self.engine, if_exists='replace', index=False, schema='raw')
        print(f"Data loaded into {table_name} table in PostgreSQL")


# Estes caminhos deviam ser relativos, para poderem funcionar noutras máquinas
# Por exemplo, se a estrutura for a seguinte
# ETL
#   |- ETL.py
#   |- README.md
#   |- SQL_Queries_Postgres.sql
#   |- .env
#   |- data/
#       |- score_csv/
#           |- table_name1.csv
#           |- table_name2.csv
# 
# Basta definir o caminho como "data/score_csv" e ".env"

path_data = r"C:\Users\tiago.costa\football-data-engineering\Data\Score_csv"

load_dotenv(r"C:\Users\tiago.costa\OneDrive - Salsajeans\Desktop\ETL\.env")

username = os.getenv('username')
password = os.getenv('password')
host = os.getenv('host')
port = os.getenv('port')
database = os.getenv('database')

# Um nome melhor para esta variável seria database_connection_string ou db_conn_string
database_url = f'postgresql://{username}:{password}@{host}:{port}/{database}'
etl = Data_Etl(path_data, database_url)


try:
    # Se estás a fazer a mesma coisa para todas as tabelas, faz sentido usar
    # um ciclo `for` para lidar com isto, em vez de repetir todo o código.
    # Algo como:

    # csv_files = ['appearances.csv', 'club_games.csv']
    # for csvf in csv_files:
    #     df = etl.load_data(csvf)  
    #     df = etl.clean_special_characters(df)
    #     df = etl.replaceNA(df)
    #     etl.load_to_postgres(df, csvf) # aqui terias que manipular um pouco o nome do CSV para retirar a extensão .csv e adicionar o sufixo _t
    #     print(f'{csvf} was transformed and load  to postgres')

    df_appearances = etl.load_data('appearances.csv')
    df_appearances = etl.clean_special_characters(df_appearances)
    df_appearances = etl.replaceNA(df_appearances)
    etl.load_to_postgres(df_appearances, 'appearances_t')
    print('df_appearances was transformed and load  to postgres')


    df_club_games = etl.load_data('club_games.csv')
    df_club_games = etl.clean_special_characters(df_club_games)
    df_club_games = etl.replaceNA(df_club_games)
    etl.load_to_postgres(df_club_games, 'club_games_t')
    print('df_club_games was transformed and load to postgres')

    df_clubs = etl.load_data('clubs.csv')
    df_clubs = etl.clean_special_characters(df_clubs)
    df_clubs = etl.replaceNA(df_clubs)
    etl.load_to_postgres(df_clubs, 'clubs_t')
    print('df_clubs was transformed and load to postgres')

    df_competitions = etl.load_data('competitions.csv')
    df_competitions = etl.clean_special_characters(df_competitions)
    df_competitions = etl.replaceNA(df_competitions)
    etl.load_to_postgres(df_competitions,  'competitions_t')
    print('df_competitions was transformed and load to postgres')

    df_game_events = etl.load_data('game_events.csv')
    df_game_events = etl.clean_special_characters(df_game_events)
    df_game_events = etl.replaceNA(df_game_events)
    etl.load_to_postgres(df_game_events, 'game_events_t')
    print('df_game_events was transformed and load to postgres')

    df_game_lineups = etl.load_data('game_lineups.csv')
    df_game_lineups = etl.clean_special_characters(df_game_lineups)
    df_game_lineups = etl.replaceNA(df_game_lineups)
    etl.load_to_postgres(df_game_lineups, 'game_lineups_t')
    print('df_game_lineups was transformed and load to postgres')

    df_games = etl.load_data('games.csv')
    df_games = etl.clean_special_characters(df_games)
    df_games = etl.replaceNA(df_games)
    etl.load_to_postgres(df_games, 'games_t')
    print('df_games was transformed and load to postgres')

    df_player_valuations = etl.load_data('player_valuations.csv')
    df_player_valuations = etl.clean_special_characters(df_player_valuations)
    df_player_valuations = etl.replaceNA(df_player_valuations)
    etl.load_to_postgres(df_player_valuations,  'player_valuations_t')
    print('df_player_valuations was transformed and load to postgres')

    df_players = etl.load_data('players.csv')
    df_players = etl.clean_special_characters(df_players)
    df_players = etl.replaceNA(df_players)
    etl.load_to_postgres(df_players, 'players_t')
    print('df_players was transformed and load to postgres')

    df_transfers = etl.load_data('transfers.csv')
    df_transfers = etl.clean_special_characters(df_transfers)
    df_transfers = etl.replaceNA(df_transfers)
    etl.load_to_postgres(df_transfers, 'transfers_t')
    print('df_transfers was transformed and load to postgres')

# Não é boa prática apanhar todas as Exceptions possíveis.
# Faz sentido usar try...except quando sabemos que pode ocorrer uma Exception
# específica e queremos lidar com ela dentro do código. Fora isso, devemos
# deixar as Exceptions ocorrer naturalmente.
except Exception as e:
     print(e)



